{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Assignment_1_(8)_(1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Plotting one sample image for each class!!"
      ],
      "metadata": {
        "id": "owzvA4sirO0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing the wandb library\n",
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "metadata": {
        "id": "xK3J0ujLrImb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "lyByaKdNsGX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.init(project=\"CS6910_DL_Assignment1\", entity=\"nomads\")"
      ],
      "metadata": {
        "id": "Af5lH0pZtfjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "P_cadGvqIJ6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HH-YkAibOaY"
      },
      "outputs": [],
      "source": [
        "#Necessary Packages\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading the Mnist_fashion data\n",
        "\n",
        "(X, Y), (X_test, Y_test) = fashion_mnist.load_data()\n"
      ],
      "metadata": {
        "id": "YyHRFA4av16M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign the tags for all the 10 classes\n",
        "Tags=['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
        "Images=[]\n",
        "for i in range(10):\n",
        "  for j in range(len(X)):\n",
        "    if(i==Y[j]):\n",
        "      Images.append(X[j])\n",
        "      break\n",
        "\n",
        "wandb.log({\"Examples\": [ wandb.Image(img, caption=caption) for img, caption in zip(Images,Tags)]})\n",
        "#Plotting the 10 classes\n",
        "\n"
      ],
      "metadata": {
        "id": "3LnFRh9EwVyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes."
      ],
      "metadata": {
        "id": "BGMHjUgiAAwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Vector Implementation and Reshaping the training and testing data set\n",
        "\n",
        "l1,l2=len(X),len(X_test)\n",
        "flat=len(X[0].flatten())\n",
        "X=(X.reshape(l1,flat))/255.0\n",
        "Y= Y.reshape(l1,1)\n",
        "X_test=(X_test.reshape(l2,flat))/255.0\n",
        "Y_test=Y_test.reshape(l2,1)\n",
        "X_train,X_validation,Y_train,Y_validation = train_test_split(X,Y, test_size=0.1, random_state=1234)\n"
      ],
      "metadata": {
        "id": "gz9iYs1SVqZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0:10].shape"
      ],
      "metadata": {
        "id": "Z1V2tAi8iFvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape,X_validation.shape,X_test.shape)\n",
        "print(Y_train.shape,Y_validation.shape,Y_test.shape)"
      ],
      "metadata": {
        "id": "G7tHEhMMzurR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining all the required Functions for Activation and Preactivation\n",
        "\n",
        "def preActivation(b,w,h):\n",
        "  \n",
        "  A= b + np.matmul(w,h)\n",
        "  return A \n",
        "\n",
        "def Activation(x,act_func):\n",
        "  if(act_func=='sigmoid'):\n",
        "    return sigmoid(x)\n",
        "  elif (act_func=='tanh'):\n",
        "    return tanh(x)\n",
        "  elif(act_func=='relu'):\n",
        "    return relu(x)\n",
        "\n",
        "#Defining the activation functions required\n",
        "  \n",
        "def sigmoid(x):\n",
        "  return 1.0/(1.0 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "#Softmax function for the output as probabilities\n",
        "def softmax(x):\n",
        "  return (np.exp(x))/(np.sum(np.exp(x)))\n",
        "\n",
        "\n",
        "#Converting class number to vector\n",
        "def to_vector(x):\n",
        "  res=np.zeros(10)\n",
        "  res[x]=1\n",
        "  return res.reshape(10,1)\n",
        "\n",
        "\n",
        "#Function to find the derivatives of the activation functions\n",
        "def derivative(func, x):\n",
        "    if func == 'sigmoid':\n",
        "        return np.multiply(sigmoid(x), 1 - sigmoid(x))\n",
        "    elif func == 'tanh':\n",
        "        return 1 - np.square(np.tanh(x))\n",
        "    elif func == 'relu':\n",
        "        return (x > 0) * (np.ones((x.shape[0],1)))\n",
        "\n",
        "#Function to calculate the losses based on the true class, estimated classes and loss function\n",
        " \n",
        "def Loss_Func(y_hat,y_true,lfunc):\n",
        "    yp = np.array(y_hat).reshape(-1)\n",
        "    yt = np.array(y_true).reshape(-1)\n",
        "    if lfunc == 'cross entropy':\n",
        "        loss = np.sum((-1)*(yt*np.log(yp))) #+ regularization/2*(sum_norm)  \n",
        "    elif lfunc == 'squared error':\n",
        "        loss = np.mean((yt-yp)**2) #+ regularization/2*(sum_norm) \n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "3T2A12Z14NOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing the weights and biases with normal random numbers\n",
        "\n",
        "def init_weights_biases(Layers,init_func,act_func):\n",
        "  W={}    #Empty dictionary to store weights\n",
        "  B={}    #Empty dictionary to store biases\n",
        "\n",
        "  #For random initialization\n",
        "  if (init_func=='random'):\n",
        "      if(act_func!= 'relu'):\n",
        "        for i in range(1,len(Layers)):\n",
        "          W[i]=np.random.randn(Layers[i],Layers[i-1])     \n",
        "          B[i]=np.random.randn(Layers[i],1)               \n",
        "      else:\n",
        "        for i in range(1,len(Layers)):\n",
        "          W[i]=np.random.randn(Layers[i],Layers[i-1])*(np.sqrt(2/(Layers[i]+Layers[i-1])))\n",
        "          B[i]=np.zeros((Layers[i],1))\n",
        "\n",
        "  #For xavier initialization\n",
        "  elif( init_func=='xavier'):\n",
        "      if (act_func != 'relu'): \n",
        "          for i in range(1, len(Layers)):\n",
        "            lower = -1/ np.sqrt(Layers[i-1])\n",
        "            upper = 1/ np.sqrt(Layers[i-1])\n",
        "            W[i] = lower + np.random.randn(Layers[i],Layers[i-1])*(upper- lower)\n",
        "            B[i] = lower + np.random.randn(Layers[i],1)*(upper - lower)\n",
        "\n",
        "      else: \n",
        "          for i in range(1,len(Layers)): \n",
        "            W[i] = np.random.randn(Layers[i],Layers[i-1])*(np.sqrt(2/(Layers[i-1])))\n",
        "            B[i]= np.random.randn(Layers[i], 1)*(np.sqrt(2/(Layers[i-1])))\n",
        "\n",
        "  return W,B\n"
      ],
      "metadata": {
        "id": "j_uKK98KV6Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing the weights and biases with zeros\n",
        "\n",
        "def init_zeros(Layers):\n",
        "  P={}    #Empty dictionary to store zero weights\n",
        "  Q={}    #Empty dictionary to store zero biases\n",
        "  for i in range(1,len(Layers)):\n",
        "        P[i]=np.zeros((Layers[i],Layers[i-1]))\n",
        "        Q[i]=np.zeros((Layers[i],1))\n",
        "  return P,Q\n"
      ],
      "metadata": {
        "id": "pOyMsolezdGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Forward Propagation\n",
        "\n",
        "def forw_prop(X,W,B,no_hidden_layers,act_func):\n",
        "    L=no_hidden_layers+1\n",
        "    A={}    #Empty dictionary to store pre-activations\n",
        "    H={}    #Empty dictionary to store activations\n",
        "    A[0]=X.reshape(784,1)   #First element stored is flattened image input\n",
        "    H[0]=X.reshape(784,1)   #First element stored is flattened image input\n",
        "    \n",
        "    for i in range(1,L):\n",
        "      A[i]=preActivation(B[i],W[i],H[i-1])    #Storing the pre-activations\n",
        "      H[i]=Activation(A[i],act_func)          #Storing the activations\n",
        "\n",
        "    A[L]=preActivation(B[L],W[L],H[L-1])      #Calculate the output layer\n",
        "    y_hat=softmax(A[L])                       #Calculate the softmax output from the output layer \n",
        "    return y_hat,A,H\n"
      ],
      "metadata": {
        "id": "KmEbG2dG9E24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Backward Propagation\n",
        "\n",
        "def back_prop(y_hat,W,B,A,H,y_true,lfunc,act_func,Layers):\n",
        "    L=len(Layers)-1\n",
        "    delta_A={}    #Creating empty dictionary to store delta A's\n",
        "    delta_H={}    #Creating empty dictionary to store delta H's\n",
        "    delta_W={}    #Creating empty dictionary to store delta W's\n",
        "    delta_B={}    #Creating empty dictionary to store delta B's\n",
        "\n",
        "    #Delta A for the last layer\n",
        "    if(lfunc=='cross entropy'):\n",
        "        delta_A[L] = -(y_true-y_hat)\n",
        "    \n",
        "    #Delta A for the hidden layers based on the type of activation functions\n",
        "    for i in range(L, 0, -1):\n",
        "        delta_W[i] = np.matmul(delta_A[i], H[i-1].T) # + regularization*W[i]\n",
        "        delta_B[i] = delta_A[i]\n",
        "        delta_H[i-1] = np.matmul(W[i].T, delta_A[i])\n",
        "        if(act_func=='sigmoid'):\n",
        "          delta_A[i-1] = np.multiply(delta_H[i-1], derivative('sigmoid',A[i-1]))\n",
        "        if(act_func=='tanh'):\n",
        "          delta_A[i-1] = np.multiply(delta_H[i-1], derivative('tanh',A[i-1]))\n",
        "        if(act_func=='relu'):\n",
        "          delta_A[i-1] = np.multiply(delta_H[i-1], derivative('relu',A[i-1]))\n",
        "\n",
        "    return delta_W,delta_B\n"
      ],
      "metadata": {
        "id": "cEuu5j0XxUhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_compute(x,y,W,B, Layers, act_func, l_func):\n",
        "    loss=0\n",
        "    #print(x.shape,y.shape)\n",
        "    for i in range(len(x)):\n",
        "        x1 = x[i].reshape(784,1)\n",
        "        yt= to_vector(y[i])\n",
        "        y_hat,a,h = forw_prop(x1,W,B,len(Layers)-2,act_func)\n",
        "        loss +=  (Loss_Func(y_hat,yt,l_func))\n",
        "        \n",
        "    loss=(loss/(len(x)))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "ufko1B0C-m0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the output using forward propagation\n",
        "def Output(x, W,B,Layers, act_func):\n",
        "    output = []\n",
        "    for i in range(len(x)):\n",
        "        y_hat, a,h = forw_prop(x[i].reshape(784,1), W,B,len(Layers)-2, act_func)\n",
        "        yp=np.argmax(y_hat,axis = 0)\n",
        "        output.append(yp)\n",
        "    return output"
      ],
      "metadata": {
        "id": "U6pZIF-rOWGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W={}\n",
        "B={}\n",
        "Layers=[784,16,16,10]\n",
        "W,B =init_weights_biases(Layers,'xavier','relu') \n",
        "y_pred = Output(X_validation,W,B,Layers, 'relu')\n",
        "print(accuracy_score(Y_validation, y_pred))\n",
        "print(loss_compute(X_validation,Y_validation,W,B, Layers,'relu','cross entropy'))"
      ],
      "metadata": {
        "id": "Y8e4m0jsNn8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generates mini batches from the training data\n",
        "'''def mini_batch_generation(train_data, train_labels, batch_size ):\n",
        "    \n",
        "    no_of_training_examples = train_data.shape[1]                 \n",
        "    batches = []\n",
        "\n",
        "    number_of_complete_minibatches = math.floor(no_of_training_examples/batch_size)\n",
        "\n",
        "    for i in range(number_of_complete_minibatches):\n",
        "        batch_input = train_data[:, i*batch_size : (i+1)*batch_size]\n",
        "        batch_output = train_labels[:, i*batch_size : (i+1)*batch_size]\n",
        "        created_batch = (batch_input, batch_output)\n",
        "        batches.append(created_batch)\n",
        "    \n",
        "    if no_of_training_examples % batch_size != 0:\n",
        "        last_batch_input = train_data[:, int(no_of_training_examples/batch_size)*batch_size : ]\n",
        "        last_batch_output = train_labels[:, int(no_of_training_examples/batch_size)*batch_size : ]\n",
        "        last_batch = (last_batch_input, last_batch_output)\n",
        "        batches.append(last_batch)\n",
        "    \n",
        "    return batches'''"
      ],
      "metadata": {
        "id": "JO1livnMqJZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stochastic Gradient Descent\n",
        "\n",
        "def StochasticGD(x,y,epochs,eta,batch,no_hidden_layers,hidden_layer_size,act_func,init_func,l_func):\n",
        "  #Store the size of the layers\n",
        "  Layers=[784] + [hidden_layer_size]*no_hidden_layers + [10]\n",
        "  #Empty list to store total loss\n",
        "  total_loss=[]\n",
        "  \n",
        "  #initialize W,B\n",
        "  W,B=init_weights_biases(Layers,init_func,act_func)\n",
        "\n",
        "  for i in range(epochs):\n",
        "      #initialize dW,dB to zeros\n",
        "      dW,dB=init_zeros(Layers)\n",
        "\n",
        "      #rd=np.arange(len(X_train))\n",
        "      #np.random.shuffle(rd)\n",
        "      count=0\n",
        "      st=en=0\n",
        "\n",
        "      for j in range(len(x)):\n",
        "          count+=1\n",
        "          #idx=rd[j]\n",
        "          #xt,yt=X_train[idx].reshape(784,1),Y_train[idx]\n",
        "          xt,yt=x[j].reshape(784,1),y[j]\n",
        "          #Perform forward prop to get predictions, activations and pre-activations\n",
        "          y_hat,A,H=forw_prop(xt,W,B,len(Layers)-2,act_func)\n",
        "          #Convert acutal class to vector\n",
        "          y_true=to_vector(yt)\n",
        "          #Calculate the gradients wrt weights and biases\n",
        "          grad_w,grad_b=back_prop(y_hat,W,B,A,H,y_true,l_func,act_func,Layers)\n",
        "\n",
        "          for l in range(1,len(Layers)):\n",
        "              #Add the gradients to dW, dB\n",
        "              dW[l]+= grad_w[l]\n",
        "              dB[l]+= grad_b[l]\n",
        "          \n",
        "          #When one batch is complete\n",
        "          if (count % batch == 0):\n",
        "              en=count\n",
        "\n",
        "              for l in range(1,len(Layers)):\n",
        "                  #Update the weights and biases layer-wise\n",
        "                  W[l]-=eta*dW[l]/batch   \n",
        "                  B[l]-=eta*dB[l]/batch\n",
        "              \n",
        "              total_loss.append(loss_compute(x[st:en],y[st:en],W,B, Layers,act_func,l_func))\n",
        "              st=en\n",
        "            \n",
        "              dW,dB=init_zeros(Layers)\n",
        "      \n",
        "      #Predictions on validation data\n",
        "      yp_val = Output(X_validation, W,B,Layers, act_func)\n",
        "      #Validation accuracy \n",
        "      acc_val = accuracy_score(Y_validation, yp_val)\n",
        "      #Outputs on training data\n",
        "      yp_train = Output(x, W,B,Layers, act_func)\n",
        "      #Training accuracy\n",
        "      acc_train = accuracy_score(y, yp_train)\n",
        "      #Loss in validation\n",
        "      val_loss = loss_compute(X_validation,Y_validation,W,B, Layers, act_func, l_func) \n",
        "      #Loss in training\n",
        "      train_loss = loss_compute(x,y,W,B, Layers, act_func, l_func)\n",
        "\n",
        "      #Making plots in wandb\n",
        "      wandb.log({\"val_acc\": acc_val})\n",
        "      wandb.log({'epochs': i})\n",
        "      wandb.log({'train_acc': acc_train})\n",
        "      wandb.log({'train_loss': train_loss})\n",
        "      wandb.log({'val_loss': val_loss})\n",
        "\n",
        "  return W,B,total_loss \n",
        "     \n",
        "  "
      ],
      "metadata": {
        "id": "yB-40i_oqEk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ws,Bs,l=StochasticGD(X_train,Y_train,5,0.1,64,2,16,'tanh','xavier','cross entropy')"
      ],
      "metadata": {
        "id": "ixvWr8D6bOZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''y_pred = Output(X_validation,Ws,Bs,[784,16,16,10], 'tanh')\n",
        "print(accuracy_score(Y_validation, y_pred))'''\n"
      ],
      "metadata": {
        "id": "U6Mn17ZOGhPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''plt.plot(l)\n",
        "plt.show()'''"
      ],
      "metadata": {
        "id": "WrL2h4Tbj-Q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Momentum Based Gradient Descent\n",
        "\n",
        "def MomentumBasedGD(x,y,epochs,eta,gamma,batch,no_hidden_layers,hidden_layer_size,act_func,init_func,l_func):\n",
        "  #Store the size of the layers\n",
        "  Layers=[784] + [hidden_layer_size]*no_hidden_layers + [10]\n",
        "  #Empty list to store total loss\n",
        "  total_loss=[]\n",
        "  \n",
        "  #initialize W,B\n",
        "  W,B=init_weights_biases(Layers,init_func,act_func)\n",
        "\n",
        "  #initialize vW,vB\n",
        "  vW,vB=init_zeros(Layers)\n",
        "\n",
        "  #initialize prev_vW,prev_vB\n",
        "  prev_vW,prev_vB=init_zeros(Layers)\n",
        "\n",
        "\n",
        "  for i in range(epochs):\n",
        "      \n",
        "      #initialize dW,dB\n",
        "      dW,dB=init_zeros(Layers)\n",
        "\n",
        "      #rd=np.arange(len(X_train))\n",
        "      #np.random.shuffle(rd)\n",
        "      count=0\n",
        "      st=en=0\n",
        "\n",
        "      for j in range(len(x)):\n",
        "          count+=1\n",
        "          #idx=rd[j]\n",
        "          #xt,yt=X_train[idx].reshape(784,1),Y_train[idx]\n",
        "          xt,yt=x[j].reshape(784,1),y[j]\n",
        "          #Perform forward prop to get predictions, activations and pre-activations\n",
        "          y_hat,A,H=forw_prop(xt,W,B,len(Layers)-2,act_func)\n",
        "          #Convert actual class to vector \n",
        "          y_true=to_vector(yt)\n",
        "          #Get the gradients wrt weights and biases\n",
        "          grad_w,grad_b=back_prop(y_hat,W,B,A,H,y_true,l_func,act_func,Layers)\n",
        "\n",
        "          for l in range(1,len(Layers)):\n",
        "              #Add gradients to dW, dB layerwise\n",
        "              dW[l]+= grad_w[l]\n",
        "              dB[l]+= grad_b[l]\n",
        "\n",
        "          #Batch is complete\n",
        "          if (count % batch == 0):\n",
        "              en=count\n",
        "              for l in range(1,len(Layers)):\n",
        "                  #Update(t) = gamma * Update(t-1) + eta * momentum\n",
        "                  vW[l]=gamma*prev_vW[l] + eta*dW[l]/batch\n",
        "                  vB[l]=gamma*prev_vB[l] + eta*dB[l]/batch\n",
        "                  #Update the weights and biases \n",
        "                  W[l]-=vW[l]\n",
        "                  B[l]-=vB[l]\n",
        "                  #Assign last update as prev update\n",
        "                  prev_vW[l]=vW[l]\n",
        "                  prev_vB[l]=vB[l]\n",
        "              #Append to loss\n",
        "              total_loss.append(loss_compute(x[st:en],y[st:en],W,B, Layers,act_func,l_func))\n",
        "              st=en\n",
        "\n",
        "              dW,dB=init_zeros(Layers)\n",
        "      \n",
        "      #Predictions on validation data\n",
        "      yp_val = Output(X_validation, W,B,Layers, act_func)\n",
        "      #Accuracy on validation data\n",
        "      acc_val = accuracy_score(Y_validation, yp_val)\n",
        "      #Predictions on training data\n",
        "      yp_train = Output(x, W,B,Layers, act_func)\n",
        "      #Accuracy on training data\n",
        "      acc_train = accuracy_score(y, yp_train)\n",
        "      #Loss in validation\n",
        "      val_loss = loss_compute(X_validation,Y_validation,W,B, Layers, act_func, l_func) \n",
        "      #Loss in training\n",
        "      train_loss = loss_compute(x,y,W,B, Layers, act_func, l_func)\n",
        "\n",
        "      #Making plots in wandb\n",
        "      wandb.log({\"val_acc\": acc_val})\n",
        "      wandb.log({'epochs': i})\n",
        "      wandb.log({'train_acc': acc_train})\n",
        "      wandb.log({'train_loss': train_loss})\n",
        "      wandb.log({'val_loss': val_loss})\n",
        "\n",
        "  return W,B,total_loss\n"
      ],
      "metadata": {
        "id": "DTmwnjIviYbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ws,Bs,l=StochasticGD(X_train,Y_train,2,0.1,64,2,16,'tanh','xavier','cross entropy')\n",
        "#Wm,Bm,lm=MomentumBasedGD(X_train,Y_train,2,0.1,0.9,64,2,16,'tanh','xavier','cross entropy')"
      ],
      "metadata": {
        "id": "ta13DRfEJ7Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''y_pred = Output(X_validation,Wm,Bm,[784,16,16,10], 'sigmoid')\n",
        "print(accuracy_score(Y_validation, y_pred))\n",
        "plt.plot(lm)\n",
        "plt.show()'''"
      ],
      "metadata": {
        "id": "iLtAku93gKqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Nesterov Accelerated Gradient Descent\n",
        "\n",
        "def NesterovAGD(x,y,epochs,eta,gamma,batch,no_hidden_layers,hidden_layer_size,act_func,init_func,l_func):\n",
        "  #Store the size of the layers\n",
        "  Layers=[784] + [hidden_layer_size]*no_hidden_layers + [10]\n",
        "  #Empty list to store total loss\n",
        "  total_loss=[]\n",
        "  \n",
        "  #initialize W,B\n",
        "  W,B=init_weights_biases(Layers,init_func,act_func)\n",
        "\n",
        "  #initialize vW,vB\n",
        "  vW,vB=init_zeros(Layers)\n",
        "\n",
        "  #initialize prev_vW,prev_vB\n",
        "  prev_vW,prev_vB=init_zeros(Layers)\n",
        "\n",
        "  #initialize look_ahead_vW,look_ahead_vB\n",
        "  look_ahead_W,look_ahead_B=init_zeros(Layers)\n",
        "\n",
        "  for l in range(1,len(Layers)):\n",
        "      look_ahead_W[l]=W[l]-gamma*prev_vW[l]\n",
        "      look_ahead_B[l]=B[l]-gamma*prev_vB[l]\n",
        "\n",
        "  for i in range(epochs):\n",
        "      \n",
        "      #initialize dW,dB\n",
        "      dW,dB=init_zeros(Layers)\n",
        "\n",
        "      #rd=np.arange(len(X_train))\n",
        "      #np.random.shuffle(rd)\n",
        "      count=0\n",
        "      st=en=0\n",
        "\n",
        "      for j in range(len(x)):\n",
        "          count+=1\n",
        "          #idx=rd[j]\n",
        "          #xt,yt=X_train[idx].reshape(784,1),Y_train[idx]\n",
        "          xt,yt=x[j].reshape(784,1),y[j]\n",
        "          #Perform forward prop to get predictions, activations and pre-activations\n",
        "          y_hat,A,H=forw_prop(xt,look_ahead_W,look_ahead_B,len(Layers)-2,act_func)\n",
        "          #Convert acutal class to vector\n",
        "          y_true=to_vector(yt)\n",
        "          #Calculate the gradients wrt weights and biases\n",
        "          grad_w,grad_b=back_prop(y_hat,look_ahead_W,look_ahead_B,A,H,y_true,l_func,act_func,Layers)\n",
        "\n",
        "          for l in range(1,len(Layers)):\n",
        "              #Add gradients to dW, dB layerwise\n",
        "              dW[l]+= grad_w[l]\n",
        "              dB[l]+= grad_b[l]\n",
        "\n",
        "          #Batch is complete\n",
        "          if (count % batch == 0):\n",
        "              en=count \n",
        "              for l in range(1,len(Layers)):\n",
        "                  #Update(t) = gamma * Update(t-1) + eta * momentum\n",
        "                  vW[l]=gamma*prev_vW[l] + eta*dW[l]/batch\n",
        "                  vB[l]=gamma*prev_vB[l] + eta*dB[l]/batch\n",
        "                  #Update the weights\n",
        "                  W[l]-=vW[l]\n",
        "                  B[l]-=vB[l]\n",
        "                  #Assign current update as preious update\n",
        "                  prev_vW[l]=vW[l]\n",
        "                  prev_vB[l]=vB[l]\n",
        "                  #Calculate look ahead point\n",
        "                  look_ahead_W[l]=W[l]-gamma*prev_vW[l]\n",
        "                  look_ahead_B[l]=B[l]-gamma*prev_vB[l]\n",
        "\n",
        "              #Append loss to the total_loss list\n",
        "              total_loss.append(loss_compute(x[st:en],y[st:en],W,B, Layers,act_func,l_func))\n",
        "              st=en\n",
        "\n",
        "              dW,dB=init_zeros(Layers)\n",
        "      \n",
        "\n",
        "      #Predictions on validation data\n",
        "      yp_val = Output(X_validation, W,B,Layers, act_func)\n",
        "      #Accuracy on validation data\n",
        "      acc_val = accuracy_score(Y_validation, yp_val)\n",
        "      #Output in training data\n",
        "      yp_train = Output(x, W,B,Layers, act_func)\n",
        "      #Accuracy in training data\n",
        "      acc_train = accuracy_score(y, yp_train)\n",
        "      #Loss in validation\n",
        "      val_loss = loss_compute(X_validation,Y_validation,W,B, Layers, act_func, l_func) \n",
        "      #Loss in training\n",
        "      train_loss = loss_compute(x,y,W,B, Layers, act_func, l_func)\n",
        "      #Making plots in wandb\n",
        "      wandb.log({\"val_acc\": acc_val})\n",
        "      wandb.log({'epochs': i})\n",
        "      wandb.log({'train_acc': acc_train})\n",
        "      wandb.log({'train_loss': train_loss})\n",
        "      wandb.log({'val_loss': val_loss})\n",
        "  return W,B,total_loss"
      ],
      "metadata": {
        "id": "pORieOTi6gV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Wn,Bn,ln=NesterovAGD(X_train,Y_train,1,0.1,0.9,64,2,16,'sigmoid','random','cross entropy')'''"
      ],
      "metadata": {
        "id": "oqpLW9DYKBut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''y_pred = Output(X_validation,Wn,Bn,[784,16,16,10], 'sigmoid')\n",
        "print(accuracy_score(Y_validation, y_pred))\n",
        "plt.plot(ln)\n",
        "plt.show()'''"
      ],
      "metadata": {
        "id": "Q5s766zJjRVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gradient Descent with RMSprop:\n",
        "\n",
        "def RMSprop(x,y,epochs,eta,beta,eps,batch,no_hidden_layers,hidden_layer_size,act_func,init_func,l_func):\n",
        "  #Store the size of the layers\n",
        "  Layers=[784] + [hidden_layer_size]*no_hidden_layers + [10]\n",
        "  #Empty list to store total loss\n",
        "  total_loss=[]\n",
        "  \n",
        "  #initialize W,B\n",
        "  W,B=init_weights_biases(Layers,init_func,act_func)\n",
        "\n",
        "  #initialize vW,vB\n",
        "  vW,vB=init_zeros(Layers)\n",
        "\n",
        "  #initialize prev_vW,prev_vB\n",
        "  prev_vW,prev_vB=init_zeros(Layers)\n",
        "\n",
        "  for i in range(epochs):\n",
        "\n",
        "      #initialize dW,dB\n",
        "      dW,dB=init_zeros(Layers)\n",
        "\n",
        "      #rd=np.arange(len(X_train))\n",
        "      #np.random.shuffle(rd)\n",
        "      count=0\n",
        "      st=en=0\n",
        "\n",
        "      for j in range(len(x)):\n",
        "          count+=1\n",
        "          #idx=rd[j]\n",
        "          #xt,yt=X_train[idx].reshape(784,1),Y_train[idx]\n",
        "          xt,yt=x[j].reshape(784,1),y[j]\n",
        "          #Perform forward propagation to get y_hat, activations, and pre-activations\n",
        "          y_hat,A,H=forw_prop(xt,W,B,len(Layers)-2,act_func)\n",
        "          #Convert actual class to vector\n",
        "          y_true=to_vector(yt)\n",
        "          #Calculate the gradients wrt to weights and biases\n",
        "          grad_w,grad_b=back_prop(y_hat,W,B,A,H,y_true,l_func,act_func,Layers)\n",
        "          for l in range(1,len(Layers)):\n",
        "              #Add gradients to dW, dB layerwise\n",
        "              dW[l]+= grad_w[l]\n",
        "              dB[l]+= grad_b[l]\n",
        "          \n",
        "          #Batch is complete\n",
        "          if(count % batch == 0):\n",
        "              en=count\n",
        "              for l in range(1,len(Layers)):\n",
        "                  #Update step\n",
        "                  vW[l]=beta*prev_vW[l] + (1-beta)*(dW[l]**2)/batch\n",
        "                  vB[l]=beta*prev_vB[l] + (1-beta)*(dB[l]**2)/batch\n",
        "                  #Update the weights and biases\n",
        "                  W[l]-=(eta/(np.sqrt(vW[l]+eps)))*(dW[l])/batch\n",
        "                  B[l]-=(eta/(np.sqrt(vB[l]+eps)))*(dB[l])/batch\n",
        "                  #Assign current update as previous update\n",
        "                  prev_vW[l]=vW[l]\n",
        "                  prev_vB[l]=vB[l]\n",
        "\n",
        "              #Append loss to the total_loss list\n",
        "              total_loss.append(loss_compute(x[st:en],y[st:en],W,B, Layers,act_func,l_func))\n",
        "              st=en\n",
        "\n",
        "              dW,dB=init_zeros(Layers)\n",
        "\n",
        "      #Predictions on validation data\n",
        "      yp_val = Output(X_validation, W,B,Layers, act_func)\n",
        "      #Accuracy on validation data\n",
        "      acc_val = accuracy_score(Y_validation, yp_val)\n",
        "      #Predictions on train data\n",
        "      yp_train = Output(x, W,B,Layers, act_func)\n",
        "      #Accuracy on train data\n",
        "      acc_train = accuracy_score(y, yp_train)\n",
        "      #Loss in validation\n",
        "      val_loss = loss_compute(X_validation,Y_validation,W,B, Layers, act_func, l_func) \n",
        "      #Loss in training\n",
        "      train_loss = loss_compute(x,y,W,B, Layers, act_func, l_func)\n",
        "      #Making plots in wandb\n",
        "      wandb.log({\"val_acc\": acc_val})\n",
        "      wandb.log({'epochs': i})\n",
        "      wandb.log({'train_acc': acc_train})\n",
        "      wandb.log({'train_loss': train_loss})\n",
        "      wandb.log({'val_loss': val_loss})\n",
        "\n",
        "  return W,B,total_loss"
      ],
      "metadata": {
        "id": "kGfjSjlsNgtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Wr,Br,lr=RMSprop(X_train,Y_train,5,0.1,0.9,1e-8,16,4,64,'sigmoid','random','cross entropy')"
      ],
      "metadata": {
        "id": "TdiAmDu7KErF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''y_pred = Output(X_validation,Wr,Br,[784,16,16,10], 'sigmoid')\n",
        "print(accuracy_score(Y_validation, y_pred))\n",
        "plt.plot(lr)\n",
        "plt.show()'''"
      ],
      "metadata": {
        "id": "wlMRR7Tr4qD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gradient Descent with Adam:\n",
        "\n",
        "def Adam(x,y,epochs,eta,beta1,beta2,eps,batch,no_hidden_layers,hidden_layer_size,act_func,init_func,l_func):\n",
        "  #Store the size of the layers\n",
        "  Layers=[784] + [hidden_layer_size]*no_hidden_layers + [10]\n",
        "  #Empty list to store total loss\n",
        "  total_loss=[]\n",
        "  \n",
        "  t=0\n",
        "  #initialize W,B\n",
        "  W,B=init_weights_biases(Layers,init_func,act_func)\n",
        "\n",
        "  #initialize vW,vB\n",
        "  vW,vB=init_zeros(Layers)\n",
        "\n",
        "  #initialize mW,mB\n",
        "  mW,mB=init_zeros(Layers)\n",
        "\n",
        "  #initialize prev_mW,prev_mB\n",
        "  prev_mW,prev_mB=init_zeros(Layers)\n",
        "\n",
        "  #initialize prev_vW,prev_vB\n",
        "  prev_vW,prev_vB=init_zeros(Layers)\n",
        "\n",
        "  #initialize mW_hat,mB_hat\n",
        "  mW_hat,mB_hat=init_zeros(Layers)\n",
        "\n",
        "  #initialize vW_hat,vB_hat\n",
        "  vW_hat,vB_hat=init_zeros(Layers)\n",
        "\n",
        "  for i in range(epochs):\n",
        "\n",
        "      #initialize dW,dB\n",
        "      dW,dB=init_zeros(Layers)\n",
        "\n",
        "      #rd=np.arange(len(X_train))\n",
        "      #np.random.shuffle(rd)\n",
        "      count=0\n",
        "      st=en=0\n",
        "\n",
        "      for j in range(len(X_train)):\n",
        "          count+=1\n",
        "          #idx=rd[j]\n",
        "          #xt,yt=X_train[idx].reshape(784,1),Y_train[idx]\n",
        "          xt,yt=x[j].reshape(784,1),y[j]\n",
        "          #Perform forward propagation to get y_hat, Activations and pre-activations\n",
        "          y_hat,A,H=forw_prop(xt,W,B,len(Layers)-2,act_func)\n",
        "          #Convert true class to vector form \n",
        "          y_true=to_vector(yt)\n",
        "          #Calculate the gradients wrt weights and biases\n",
        "          grad_w,grad_b=back_prop(y_hat,W,B,A,H,y_true,l_func,act_func,Layers)\n",
        "\n",
        "          #Add the gradients to dW, dB layerwise\n",
        "          for l in range(1,len(Layers)):\n",
        "              dW[l]+= grad_w[l]\n",
        "              dB[l]+= grad_b[l]\n",
        "\n",
        "          #Batch is complete\n",
        "          if(count % batch == 0):\n",
        "              en=count\n",
        "              t+=1\n",
        "\n",
        "              for l in range(1,len(Layers)):\n",
        "                  mW[l]=beta1*prev_mW[l] + (1-beta1)*dW[l]/batch\n",
        "                  mB[l]=beta1*prev_mB[l] + (1-beta1)*dB[l]/batch\n",
        "\n",
        "                  vW[l]=beta2*prev_vW[l] + (1-beta2)*(dW[l]**2)/batch\n",
        "                  vB[l]=beta2*prev_vB[l] + (1-beta2)*(dB[l]**2)/batch\n",
        "\n",
        "                  mW_hat[l]=mW[l]/(1-np.power(beta1,t))\n",
        "                  mB_hat[l]=mB[l]/(1-np.power(beta1,t))\n",
        "\n",
        "                  vW_hat[l]=vW[l]/(1-np.power(beta2,t))\n",
        "                  vB_hat[l]=vB[l]/(1-np.power(beta2,t))\n",
        "\n",
        "                  W[l]-=(eta/(np.sqrt(vW_hat[l]+eps)))*mW_hat[l]\n",
        "                  B[l]-=(eta/(np.sqrt(vB_hat[l]+eps)))*mB_hat[l]\n",
        "\n",
        "                  prev_mW[l]=mW[l]\n",
        "                  prev_mB[l]=mB[l]\n",
        "\n",
        "                  prev_vW[l]=vW[l]\n",
        "                  prev_vB[l]=vB[l]\n",
        "\n",
        "              total_loss.append(loss_compute(x[st:en],y[st:en],W,B, Layers,act_func,l_func))\n",
        "              st=en\n",
        "\n",
        "              dW,dB=init_zeros(Layers)\n",
        "      \n",
        "      #Predictions on validation data\n",
        "      yp_val = Output(X_validation, W,B,Layers, act_func)\n",
        "      #Accracy on validation data\n",
        "      acc_val = accuracy_score(Y_validation, yp_val)\n",
        "      #Predictions on train data\n",
        "      yp_train = Output(x, W,B,Layers, act_func)\n",
        "      #Accuracy on train data\n",
        "      acc_train = accuracy_score(y, yp_train)\n",
        "      #Loss in validation\n",
        "      val_loss = loss_compute(X_validation,Y_validation,W,B, Layers, act_func, l_func) \n",
        "      #Loss in training\n",
        "      train_loss = loss_compute(x,y,W,B, Layers, act_func, l_func)\n",
        "      #Making plots in wandb\n",
        "      wandb.log({\"val_acc\": acc_val})\n",
        "      wandb.log({'epochs': i})\n",
        "      wandb.log({'train_acc': acc_train})\n",
        "      wandb.log({'train_loss': train_loss})\n",
        "      wandb.log({'val_loss': val_loss})\n",
        "\n",
        "  return W,B,total_loss"
      ],
      "metadata": {
        "id": "5rpoWWothawW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Wa,Ba,la=Adam(X_train,Y_train,5,0.1,0.9,0.999,1e-8,16,4,64,'sigmoid','random','cross entropy')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sWrYjpLe6ayz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''y_pred = Output(X_validation,Wa,Ba,[784,16,16,10], 'sigmoid')\n",
        "print(accuracy_score(Y_validation, y_pred))\n",
        "plt.plot(la)\n",
        "plt.show()'''"
      ],
      "metadata": {
        "id": "pLDz_cnw8QtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gradient Descent with Nadam:\n",
        "\n",
        "def Nadam(x,y,epochs,eta,gamma,beta1,beta2,eps,batch,no_hidden_layers,hidden_layer_size,act_func,init_func,l_func):\n",
        "  #Store the size of the layers\n",
        "  Layers=[784] + [hidden_layer_size]*no_hidden_layers + [10]\n",
        "  #Empty list to store total loss\n",
        "  total_loss=[]\n",
        "  \n",
        "  t=0\n",
        "  #initialize W,B\n",
        "  W,B=init_weights_biases(Layers,init_func,act_func)\n",
        "\n",
        "  #initialize vW,vB\n",
        "  vW,vB=init_zeros(Layers)\n",
        "\n",
        "  #initialize mW,mB\n",
        "  mW,mB=init_zeros(Layers)\n",
        "\n",
        "  #initialize prev_mW,prev_mB\n",
        "  prev_mW,prev_mB=init_zeros(Layers)\n",
        "\n",
        "  #initialize prev_vW,prev_vB\n",
        "  prev_vW,prev_vB=init_zeros(Layers)\n",
        "\n",
        "  #initialize mW_hat,mB_hat\n",
        "  mW_hat,mB_hat=init_zeros(Layers)\n",
        "\n",
        "  #initialize vW_hat,vB_hat\n",
        "  vW_hat,vB_hat=init_zeros(Layers)\n",
        "\n",
        "  #initialize look_ahead_vW,look_ahead_vB\n",
        "  look_ahead_W,look_ahead_B=init_zeros(Layers)\n",
        "\n",
        "  for l in range(1,len(Layers)):\n",
        "      look_ahead_W[l]=W[l]-gamma*prev_vW[l]\n",
        "      look_ahead_B[l]=B[l]-gamma*prev_vB[l]\n",
        " \n",
        "\n",
        "  for i in range(epochs):\n",
        "\n",
        "      #initialize dW,dB\n",
        "      dW,dB=init_zeros(Layers)\n",
        "\n",
        "      #rd=np.arange(len(X_train))\n",
        "      #np.random.shuffle(rd)\n",
        "      count=0\n",
        "      st=en=0\n",
        "\n",
        "      for j in range(len(x)):\n",
        "          count+=1\n",
        "          #idx=rd[j]\n",
        "          #xt,yt=X_train[idx].reshape(784,1),Y_train[idx]\n",
        "          xt,yt=x[j].reshape(784,1),y[j]\n",
        "          y_hat,A,H=forw_prop(xt,look_ahead_W,look_ahead_B,len(Layers)-2,act_func)\n",
        "          y_true=to_vector(yt)\n",
        "          grad_w,grad_b=back_prop(y_hat,look_ahead_W,look_ahead_B,A,H,y_true,l_func,act_func,Layers)\n",
        "          for l in range(1,len(Layers)):\n",
        "              dW[l]+= grad_w[l]\n",
        "              dB[l]+= grad_b[l]\n",
        "\n",
        "          if(count % batch == 0):\n",
        "              en=count \n",
        "              t+=1\n",
        "\n",
        "              for l in range(1,len(Layers)):\n",
        "                  mW[l]=beta1*prev_mW[l] + (1-beta1)*dW[l]/batch\n",
        "                  mB[l]=beta1*prev_mB[l] + (1-beta1)*dB[l]/batch\n",
        "\n",
        "                  vW[l]=beta2*prev_vW[l] + (1-beta2)*(dW[l]**2)/batch\n",
        "                  vB[l]=beta2*prev_vB[l] + (1-beta2)*(dB[l]**2)/batch\n",
        "\n",
        "                  mW_hat[l]=mW[l]/(1-np.power(beta1,t))\n",
        "                  mB_hat[l]=mB[l]/(1-np.power(beta1,t))\n",
        "\n",
        "                  vW_hat[l]=vW[l]/(1-np.power(beta2,t))\n",
        "                  vB_hat[l]=vB[l]/(1-np.power(beta2,t))\n",
        "\n",
        "                  W[l]-=(eta/(np.sqrt(vW_hat[l]+eps)))*mW_hat[l]\n",
        "                  B[l]-=(eta/(np.sqrt(vB_hat[l]+eps)))*mB_hat[l]\n",
        "\n",
        "                  prev_mW[l]=mW[l]\n",
        "                  prev_mB[l]=mB[l]\n",
        "\n",
        "                  prev_vW[l]=vW[l]\n",
        "                  prev_vB[l]=vB[l]\n",
        "\n",
        "                  look_ahead_W[l]=W[l]-gamma*prev_vW[l]\n",
        "                  look_ahead_B[l]=B[l]-gamma*prev_vB[l]\n",
        "\n",
        "              #total_loss.append(loss_compute(x[st:en],y[st:en],W,B, Layers,act_func,l_func))\n",
        "              st=en\n",
        "\n",
        "              dW,dB=init_zeros(Layers)\n",
        "\n",
        "      #Predictions on validation data\n",
        "      yp_val = Output(X_validation, W,B,Layers, act_func)\n",
        "      #Accracy on validation data\n",
        "      acc_val = accuracy_score(Y_validation, yp_val)\n",
        "      #Predictions on train data\n",
        "      yp_train = Output(x, W,B,Layers, act_func)\n",
        "      #Accuracy on train data\n",
        "      acc_train = accuracy_score(y, yp_train)\n",
        "      #Loss in validation\n",
        "      val_loss = loss_compute(X_validation,Y_validation,W,B, Layers, act_func, l_func) \n",
        "      #Loss in training\n",
        "      train_loss = loss_compute(x,y,W,B, Layers, act_func, l_func)\n",
        "      #Making plots in wandb\n",
        "      wandb.log({\"val_acc\": acc_val})\n",
        "      wandb.log({'epochs': i})\n",
        "      wandb.log({'train_acc': acc_train})\n",
        "      wandb.log({'train_loss': train_loss})\n",
        "      wandb.log({'val_loss': val_loss})\n",
        "\n",
        "  return W,B,total_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "fQqpGiDUlGGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Wna,Bna,lna=Nadam(X_train,Y_train,5,0.1,0.9,0.9,0.999,1e-8,16,4,64,'sigmoid','random','cross entropy')"
      ],
      "metadata": {
        "id": "iHfUhIJAm84O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''y_pred = Output(X_validation,Wna,Bna,[784,16,16,10], 'sigmoid')\n",
        "print(accuracy_score(Y_validation, y_pred))\n",
        "plt.plot(lna)\n",
        "plt.show()'''"
      ],
      "metadata": {
        "id": "MeyqqT_a9U77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'early_terminate': {\n",
        "            'type': 'hyperband',\n",
        "            'min_iter': [3],\n",
        "            's': [2]\n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [5,10] #number of epochs\n",
        "        },\n",
        "        'number_hidden': {\n",
        "            'values': [3, 4, 5] #number of hidden layers\n",
        "        },\n",
        "        'hidden_inputsize': {\n",
        "            'values': [32, 64, 128] #size of every hidden layer\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values': [0, 0.0005,  0.5] #L2 regularisation\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-3, 1e-4] #values of eta\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': [ 'sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam'] #algorithms for training\n",
        "        },\n",
        "        'batch_size' : {\n",
        "            'values':[16, 32, 64] #Sizes of batches to be used\n",
        "        },\n",
        "        'weight_init': {\n",
        "            'values': ['random','xavier'] #Types of initializations\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['sigmoid','tanh','relu'] #Types of activations\n",
        "        }\n",
        "        \n",
        "        }\n",
        "}"
      ],
      "metadata": {
        "id": "54u4ZKQjrkDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a new sweep\n",
        "# Arguments:\n",
        "# param_config: the sweep config dictionary defined above\n",
        "# entity: Set the username for the sweep\n",
        "# project: Set the project name for the sweep\n",
        "sweep_id = wandb.sweep(sweep_config, entity=\"nomads\", project=\"CS6910_DL_Assignment1\")"
      ],
      "metadata": {
        "id": "XKCTH2aar78D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kdef train():\n",
        "    config_defaults = {\n",
        "        'num_of_epochs': 5,\n",
        "        'number_of_hidden_layers': 3,\n",
        "        'hidden_layer_size': 64,\n",
        "        'loss_function':'cross entropy',\n",
        "        'learning_rate': 1e-3,\n",
        "        'optimizer': 'adam',\n",
        "        'batch_size': 64,\n",
        "        'activation': 'relu',\n",
        "        'weight_initialization': 'xavier',\n",
        "        'gamma' : 0.9,\n",
        "        'epsilon' : 1e-5,\n",
        "        'beta': 0.95,\n",
        "        'beta1' : 0.9,\n",
        "        'beta2' : 0.999\n",
        "    }\n",
        "\n",
        "    wandb.init(config=config_defaults,resume=True)\n",
        "    config = wandb.config \n",
        "    wandb.run.name = \"hl_\" + str(config.number_of_hidden_layers)+ \"_layer-size_\" + str(config.hidden_layer_size) +\"lf\"+config.loss_function+ \"_bs_\"+str(config.batch_size)+\"_ac_\"+ (config.activation) + \"_init_\" + (config.weight_initialization) + \"_opt_\" + (config.optimizer)\n",
        "\n",
        "    if config.optimizer==\"sgd\":\n",
        "        StochasticGD(X_train, Y_train,config.num_of_epochs,config.learning_rate, config.batch_size, config.number_of_hidden_layers, config.hidden_layer_size,  \n",
        "              config.activation, config.weight_initialization,config.loss_function)\n",
        "    \n",
        "    elif config.optimizer==\"momentum\":  \n",
        "        MomentumBasedGD(X_train, Y_train,config.num_of_epochs,config.learning_rate,config.gamma, config.batch_size, config.number_of_hidden_layers, config.hidden_layer_size,  \n",
        "              config.activation, config.weight_initialization,config.loss_function)\n",
        "        \n",
        "    elif config.optimizer=='nesterov':\n",
        "        NesterovAGD(X_train, Y_train,config.num_of_epochs,config.learning_rate,config.gamma, config.batch_size, config.number_of_hidden_layers, config.hidden_layer_size,  \n",
        "              config.activation, config.weight_initialization,config.loss_function)\n",
        "        \n",
        "    elif config.optimizer==\"rmsprop\":\n",
        "        RMSprop(X_train, Y_train,config.num_of_epochs,config.learning_rate,config.beta,config.epsilon, config.batch_size, config.number_of_hidden_layers, config.hidden_layer_size,  \n",
        "              config.activation, config.weight_initialization,config.loss_function)\n",
        "\n",
        "    elif config.optimizer=='adam':\n",
        "        Adam(X_train, Y_train,config.num_of_epochs,config.learning_rate,config.beta1,config.beta2,config.epsilon, config.batch_size, config.number_of_hidden_layers, config.hidden_layer_size,  \n",
        "              config.activation, config.weight_initialization,config.loss_function) \n",
        "    \n",
        "    elif config.optimizer=='nadam':\n",
        "        Nadam(X_train, Y_train,config.num_of_epochs,config.learning_rate,config.gamma,config.beta1,config.beta2,config.epsilon, config.batch_size, config.number_of_hidden_layers, config.hidden_layer_size,  \n",
        "              config.activation, config.weight_initialization,config.loss_function)"
      ],
      "metadata": {
        "id": "3TVaoSk35Vla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent('4l43rew9',train,count=1)"
      ],
      "metadata": {
        "id": "a5sO0RQwg4jw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}